{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4862520,"sourceType":"datasetVersion","datasetId":2818963}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **<p style=\"font-family:newtimeroman;font-size:200%;text-align:center;color:#06445e;\">Amazon Sales Dataset EDA</p>**","metadata":{}},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Task</span>**\n### ***Exploring the Amazon Sales Dataset involves a step-by-step process. First, we clean and prepare the data to ensure it's accurate and consistent. Then, we summarize the data using descriptive statistics like averages and ranges. Next, we visualize the data with charts and graphs to see patterns and relationships. We detect outliers, which are unusual data points, and test our assumptions about the data. We divide the data into groups for better understanding and finally, we summarize our findings.*** ","metadata":{}},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Objectives</span>**\n### ***The primary objective of analyzing the Amazon Sales Dataset is delve into product categories, prices, ratings, and sales patterns to identify characteristics that resonate with consumers and propel them to purchase.***\n### ***Delve into product categories, prices, ratings, and sales patterns to identify characteristics that resonate with consumers and propel them to purchase.***\n### ***Translate insights into actionable recommendations that optimize product development, inform marketing strategies, and boost your competitive edge.***\n### ***Equip businesses with the knowledge to create products that cater to evolving consumer needs and desires.***\n### ***Craft communication strategies that resonate with specific demographics and maximize engagement.***\n### ***Facilitate a marketplace where products find their perfect match in the hearts of consumers.***","metadata":{}},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Kernel Version Used</span>**\n- ***Python 3.12.0***","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Import Libraries</p>\n### ***We will use the following libraries***\n### ***1. Pandas: Data manipulation and analysis***\n### ***2. Numpy: Numerical operations and calculations***\n### ***3. Matplotlib: Data visualization and plotting***\n### ***4. Seaborn: Enhanced data visualization and statistical graphics***\n### ***5. Scipy: Scientific computing and advanced mathematical operations***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\n# this is for jupyter notebook to show the plot in the notebook itself instead of opening a new window\n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2026-01-10T06:47:53.960901Z","execution_failed":"2026-01-10T07:37:54.929Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Data Loading and Exploration | Cleaning</p>","metadata":{}},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Load a CSV file then creating a dataframe</span>**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/amazon-sales-dataset/amazon.csv\")","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.931Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Set the option to show maximum columns</span>**","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', None) ","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.932Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Get a sneak peek of data</span>**\n### ***The purpose of a sneak peek is to get a quick overview of the data and identify any potential problems or areas of interest.***","metadata":{}},{"cell_type":"code","source":"# Let's have a look on top 5 rows of the data\ndf.head(5)","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Let's see the column names</span>**","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.933Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Let's have a look on the shape of the dataset</span>**","metadata":{}},{"cell_type":"code","source":"print(f\"The Number of Rows are {df.shape[0]}, and columns are {df.shape[1]}.\")","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Let's have a look on the columns and their data types using detailed info function</span>**","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.935Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.935Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Observation Set 1</p>\n- ***There are 1465 rows and 16 columns in the dataset.***\n- ***The data type of all columns  is object.***\n-  ***The columns in the datasets are:***\n   - ***'product_id', 'product_name', 'category', 'discounted_price',\n      'actual_price', 'discount_percentage', 'rating', 'rating_count',\n      'about_product', 'user_id', 'user_name', 'review_id', 'review_title',\n      'review_content', 'img_link', 'product_link'***\n- ***There are a few missing values in the dataset, which we will read in detail and deal with later on in the notebook.***\n","metadata":{}},{"cell_type":"markdown","source":"# **<p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Changing Data Types of Columns from object to float</p>**","metadata":{}},{"cell_type":"code","source":"# Changing the data type of discounted price and actual price\n\ndf['discounted_price'] = df['discounted_price'].str.replace(\"₹\",'')\ndf['discounted_price'] = df['discounted_price'].str.replace(\",\",'')\ndf['discounted_price'] = df['discounted_price'].astype('float64')\n\ndf['actual_price'] = df['actual_price'].str.replace(\"₹\",'')\ndf['actual_price'] = df['actual_price'].str.replace(\",\",'')\ndf['actual_price'] = df['actual_price'].astype('float64')","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.935Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing Datatype and values in Discount Percentage\n\ndf['discount_percentage'] = df['discount_percentage'].str.replace('%','').astype('float64')\n\ndf['discount_percentage'] = df['discount_percentage'] / 100","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.936Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Finding unusual string in rating column\ndf['rating'].value_counts()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.936Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the strange row\ndf.query('rating == \"|\"')","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.937Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***I got this product `rating` on Amazon by searching the provided `product_id` on their official website (amazon.in)***\n### ***The `rating` is 3.9. So, I am going to give the item `rating` a 3.9 as well.***","metadata":{}},{"cell_type":"code","source":"# Changing Rating Columns Data Type\n\ndf['rating'] = df['rating'].str.replace('|', '3.9').astype('float64')","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.938Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Changing 'rating_count' Column Data Type\n\ndf['rating_count'] = df['rating_count'].str.replace(',', '').astype('float64')","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.938Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.939Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Descriptive Statistics</p>\n### ***Descriptive statistics are a collection of quantitative measures that summarize and describe the main characteristics of a dataset.***","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.940Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Observation Set 2</p>\n- ***All columns data type was object So, I converted some column data type to float.***\n- ***There are 4 numeric as per Python coding or descriptive statistics from Python describe function***\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Dealing with the missing values</p>","metadata":{}},{"cell_type":"markdown","source":"### ***Dealing with the missing values is one of the most important part of the data wrangling process, we must deal with the missing values in order to get the correct insights from the data.***","metadata":{}},{"cell_type":"markdown","source":"\n### **<h1 align=\"center\"><span style=\"color:#06445e;\">Missing Values</span>**","metadata":{}},{"cell_type":"code","source":"df.isnull().sum().sort_values(ascending = False)","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.941Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find missing values percentage in the data\nround(df.isnull().sum() / len(df) * 100, 2).sort_values(ascending=False) ","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.942Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find total number of missing values\ndf.isnull().sum().sum()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.943Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Let's plot the missing values</span>**","metadata":{}},{"cell_type":"code","source":"# make a figure size\nplt.figure(figsize=(22, 10))\n# plot the null values in each column\nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis') ","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.943Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"        Figure-1: Heatmap of Missing Values","metadata":{}},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Let's plot the missing values by percentage</span>**","metadata":{}},{"cell_type":"code","source":"# make figure size\nplt.figure(figsize=(22, 10))\n# plot the null values by their percentage in each column\nmissing_percentage = df.isnull().sum()/len(df)*100\nmissing_percentage.plot(kind='bar')\n# add the labels\nplt.xlabel('Columns')\nplt.ylabel('Percentage')\nplt.title('Percentage of Missing Values in each Column')","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"        Figure-2: This is a percentage null values plot.\n","metadata":{}},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">We are only viewing the rows where there are null values in the column.</span>**","metadata":{}},{"cell_type":"code","source":"df[df['rating_count'].isnull()].head(5)","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Impute missing values\ndf['rating_count'] = df.rating_count.fillna(value=df['rating_count'].median())","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum().sort_values(ascending = False)","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-10T07:37:54.944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Milestone 1: We have cleaned the dataset from null values***","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Find Duplications and Analyse them</p>","metadata":{}},{"cell_type":"markdown","source":"### **<h1 align=\"center\"><span style=\"color:#06445e;\">Duplicates</span>**\n### ***Removing duplicates is one of the most important part of the data wrangling process, we must remove the duplicates in order to get the correct insights from the data.***\n### ***If you do not remove duplicates from a dataset, it can lead to incorrect insights and analysis.***\n### ***Duplicates can skew statistical measures such as mean, median, and standard deviation, and can also lead to over-representation of certain data points.***\n### ***It is important to remove duplicates to ensure the accuracy and reliability of your data analysis.***","metadata":{}},{"cell_type":"code","source":"# Find Duplicate\ndf.duplicated().any()","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-10T07:37:54.945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-10T07:37:54.945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"any_duplicates = df.duplicated(subset=['product_id', 'product_name', 'category', 'discounted_price',\n       'actual_price', 'discount_percentage', 'rating', 'rating_count',\n       'about_product', 'user_id', 'user_name', 'review_id', 'review_title',\n       'review_content', 'img_link', 'product_link']).any()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.946Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"any_duplicates","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.946Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ***Milestone 2: Hence no duplicates found***","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Data Visualization</p>","metadata":{}},{"cell_type":"markdown","source":"## **<p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Scatter Plot</p>**","metadata":{}},{"cell_type":"code","source":"# Plot actual_price vs. rating\nplt.scatter(df['actual_price'], df['rating'])\nplt.xlabel('Actual_price')\nplt.ylabel('Rating')\nplt.show()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.946Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dont show warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **<p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Histogram</p>**","metadata":{}},{"cell_type":"code","source":"# Plot distribution of actual_price\nplt.hist(df['actual_price'])\nplt.xlabel('Actual Price')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# label encode categorical variables\n\nle_product_id = LabelEncoder()\nle_category = LabelEncoder()\nle_review_id = LabelEncoder()\nle_review_content = LabelEncoder()\nle_product_name = LabelEncoder()\nle_user_name = LabelEncoder()\nle_about_product = LabelEncoder()\nle_user_id = LabelEncoder()\nle_review_title = LabelEncoder()\nle_img_link = LabelEncoder()\nle_product_link = LabelEncoder()\n\n\ndf['product_id'] = le_product_id.fit_transform(df['product_id'])\ndf['category'] = le_category.fit_transform(df['category'])\ndf['review_id'] = le_review_id.fit_transform(df['review_id'])\ndf['review_content'] = le_review_content.fit_transform(df['review_content'])\ndf['product_name'] = le_product_name.fit_transform(df['product_name'])\ndf['user_name'] = le_user_name.fit_transform(df['user_name'])\ndf['about_product'] = le_about_product.fit_transform(df['about_product'])\ndf['user_id'] = le_user_id.fit_transform(df['user_id'])\ndf['review_title'] = le_review_title.fit_transform(df['review_title'])\ndf['img_link'] = le_img_link.fit_transform(df['img_link'])\ndf['product_link'] = le_product_link.fit_transform(df['product_link'])","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **<p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Heatmap</p>**","metadata":{}},{"cell_type":"code","source":"# Plot correlations between variables\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=True)\nplt.show()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.948Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Correlation Analysis:</p>","metadata":{}},{"cell_type":"code","source":"# Calculate Pearson correlation coefficients (default in Pandas)\ncorrelation_matrix = df.corr()\n\n# Print the correlation matrix\nprint(correlation_matrix)\n\n# Create a heatmap to visualize the correlations\nsns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix (Pearson)\")\nplt.show()\n\n# Calculate Spearman correlation coefficients (for non-linear relationships)\nspearman_correlation_matrix = df.corr(method=\"spearman\")\n\n# Print the Spearman correlation matrix\nprint(spearman_correlation_matrix)\n\n# Create a heatmap to visualize the Spearman correlations\nsns.heatmap(spearman_correlation_matrix, annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Matrix (Spearman)\")\nplt.show()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.949Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate correlation coefficient between product price and sales\ncorrelation_coefficient = np.corrcoef(df['actual_price'], df['rating'])[0, 1]\n\n# Print correlation coefficient\nprint(correlation_coefficient)","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.950Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Grouping and Aggregation</p>","metadata":{}},{"cell_type":"code","source":"# Calculate mean sales by product category\ngrouped_df = df.groupby('category')['rating'].mean()\n\n# Print mean sales by product category\nprint(grouped_df)","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **<h1 align=\"center\"><span style=\"color:#06445e;\">Calculate summary statistics for groups</span>**","metadata":{}},{"cell_type":"code","source":"# Mean rating by category\nmean_sales_by_category = df.groupby('category')['rating'].mean()\nprint(mean_sales_by_category)\n\n# Median rating by review_content\nmedian_sales_by_age = df.groupby('review_content')['rating'].median()\nprint(median_sales_by_age)\n\n# Standard deviation of actual_price by product_name\nstd_price_by_brand = df.groupby('product_name')['actual_price'].std()\nprint(std_price_by_brand)","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Create pivot tables</p>","metadata":{}},{"cell_type":"code","source":"# Pivot table of rating by category and customer location\npivot_table = df.pivot_table(values='rating', index='category', columns='product_link', aggfunc='mean')\nprint(pivot_table)\n\n# Pivot table of average rating_count by customer age group and product category\npivot_table = df.pivot_table(values='rating_count', index='review_content', columns='category', aggfunc='mean')\nprint(pivot_table)","metadata":{"_kg_hide-output":true,"execution":{"execution_failed":"2026-01-10T07:37:54.953Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Statistical Tests:</p>","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats\n\n# Conduct t-test to compare rating between two categories\nt_statistic, p_value = stats.ttest_ind(df[df['category'] == 'electronics']['rating'], df[df['category'] == 'clothing']['rating'])\n\n# Print t-statistic and p-value\nprint(t_statistic, p_value)","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Chi-square test\n\n# Create a contigency table\ncontigency_table = pd.crosstab(df['actual_price'], df['rating'])\ncontigency_table","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# perform chi-square test\nchi2, p, dof, expected = stats.chi2_contingency(contigency_table)\n\n# print the results\nprint('Chi-square statistic:', chi2)\nprint('p-value:', p)\nprint('Degrees of freedom:', dof)\nprint(f\"Expected:\\n {expected}\")","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inverse transform the data\n\ndf['product_id'] = le_product_id.inverse_transform(df['product_id'])\ndf['category'] = le_category.inverse_transform(df['category'])\ndf['review_id'] = le_review_id.inverse_transform(df['review_id'])\ndf['review_content'] = le_review_content.inverse_transform(df['review_content'])\ndf['product_name'] = le_product_name.inverse_transform(df['product_name'])\ndf['user_name'] = le_user_name.inverse_transform(df['user_name'])\ndf['about_product'] = le_about_product.inverse_transform(df['about_product'])\ndf['user_id'] = le_user_id.inverse_transform(df['user_id'])\ndf['review_title'] = le_review_title.inverse_transform(df['review_title'])\ndf['img_link'] = le_img_link.inverse_transform(df['img_link'])\ndf['product_link'] = le_product_link.inverse_transform(df['product_link'])","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.955Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"font-family:newtimeroman;font-size:150%;text-align:center;color:#06445e;\">Questions and Answers</p>\n### ***These are some questions are follows:***\n\n### ***Q1: What is the average rating for each product category?***\n### ***Q2: What are the top rating_count products by category?***\n### ***Q3: What is the distribution of discounted prices vs. actual prices?***\n### ***Q4: How does the average discount percentage vary across categories?***\n### ***Q5: What are the most popular product name?***\n### ***Q6: What are the most popular product keywords?***\n### ***Q7: What are the most popular product reviews?***\n### ***Q8: What is the correlation between discounted_price and rating?***\n### ***Q9: What are the Top 5 categories based with highest ratings?***\n","metadata":{}},{"cell_type":"markdown","source":"## ***Q1: What is the average rating for each product category?***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Check the data type of the \"rating\" column\nprint(df[\"rating\"].dtype)\n\n# If the data type is not numeric, convert it to numeric\nif df[\"rating\"].dtype == \"object\":\n    df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")  # Handle potential errors\n\n# Calculate the average ratings after ensuring numeric data type\naverage_ratings = df.groupby(\"category\")[\"rating\"].mean().reset_index()\n\nprint(average_ratings)","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.956Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***Answer 1:*** \n- ***The output shows that most product `categories` have generally positive customer feedback, with `average ratings above 3.50`. However, some categories (e.g., 2 and 3) have lower ratings, suggesting potential areas for improvement. Further analysis of these categories could help identify specific reasons for lower feedback and identify potential solutions.***","metadata":{}},{"cell_type":"markdown","source":"## ***Q2: What are the top rating_count products by category?***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntop_reviewed_per_category = (\n    df.groupby(\"category\")\n    .apply(lambda x: x.nlargest(10, \"rating_count\"))\n    .reset_index(drop=True)\n)\n\nprint(top_reviewed_per_category)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"execution_failed":"2026-01-10T07:37:54.957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***Answer 2:*** \n- ***The output highlights products likely to be popular within their categories based on high `review counts`, suggesting customer interest and engagement.***\n- ***Review counts range from `9 to 15867`, implying varying levels of attention and feedback across products.***\n- ***Most listed products have `ratings above 3.5`, indicating a generally positive customer experience.***\n- ***Products with the highest `review counts` within their `categories` might be considered potential top sellers, even without direct sales data.***","metadata":{}},{"cell_type":"markdown","source":"## ***Q3: What is the distribution of discounted prices vs. actual prices?***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Create histograms\ndf[\"discounted_price\"].hist(label=\"Discounted Price\")\ndf[\"actual_price\"].hist(label=\"Actual Price\")\n\n# Calculate and analyze discount percentages\ndf[\"discount_percentage\"] = (df[\"actual_price\"] - df[\"discounted_price\"]) / df[\"actual_price\"] * 100\ndf[\"discount_percentage\"].describe()\ndf[\"discount_percentage\"].hist(label=\"Discount Percentage\")\n","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***Answer 3:***\n- ***The output shows that `discounted prices` are generally lower than `actual prices`, with a `median discounted price of $200` and a `median actual price of $400`.***\n- ***The `discount percentage` distribution is skewed to the left, with most products having a discount of `30% or less`.***\n- ***The output suggests that there may be opportunities to `increase discounted prices or discount percentages` to attract more customers.***","metadata":{}},{"cell_type":"markdown","source":"## ***Q4: How does the average discount percentage vary across categories?***","metadata":{}},{"cell_type":"code","source":"# Calculate average discount percentage per category\navg_discount_per_category = df.groupby('category')['discount_percentage'].mean()\n\n# Display results\nprint(avg_discount_per_category)\n\n# Optional: Visualization\nsns.barplot(x=avg_discount_per_category.index, y=avg_discount_per_category.values)\nplt.xlabel(\"Category\")\nplt.ylabel(\"Average Discount Percentage\")\nplt.show()","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***Answer 4:***\n- ***`Average discount percentages` vary widely across categories, ranging from `0% to 78.39%`.***\n- ***Categories 1 and 3 stand out with notably higher `average discounts (78.39% and 56.34%)`, suggesting potential factors like clearance efforts, high competition, or lower-profit margins.***\n- ***`Categories 0, 206, 207, 210` have `average discounts of 0%`, indicating consistent pricing or strong demand for products within those categories.***\n- ***Other categories exhibit varying `discount percentages`, likely reflecting diverse pricing strategies and market dynamics.***","metadata":{}},{"cell_type":"markdown","source":"## ***Q5: What are the most popular product name?***","metadata":{}},{"cell_type":"code","source":"# Count occurrences of product names\nproduct_counts = df[\"product_name\"].value_counts()\n\n# Sort in descending order and display top results\nprint(product_counts.sort_values(ascending=False).head(10))","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***Answer 5:***\n- ***Fire-Boltt Ninja Call Pro Plus Smart Watch is the most popular product, followed by Fire-Boltt Phoenix Smart Watch.***\n- ***Smart Watches and Charging Cables are the most popular product categories.***\n- ***Multiple brands are represented, with boAt appearing twice.***\n- ***Fast charging, durability, and functionality are key features.***\n- ***Popularity is relatively evenly distributed beyond the leading product.***","metadata":{}},{"cell_type":"markdown","source":"## ***Q6: What are the most popular product keywords?*** ","metadata":{}},{"cell_type":"code","source":"def extract_keywords(product_name):\n  \"\"\"Extracts keywords from a product name, handling potential numbers.\"\"\"\n  if isinstance(product_name, str):  # Check if it's a string\n    keywords = product_name.lower().split()  # Split into words and lowercase\n    keywords = [word for word in keywords if word.isalpha()]  # Remove non-alphabetical characters\n  else:\n    keywords = []  # Handle non-string values (e.g., integers) by returning an empty list\n  return keywords\n\n# Apply the function to extract keywords\ndf[\"keywords\"] = df[\"product_name\"].apply(extract_keywords)\n\n# Flatten the list of keywords\nall_keywords = [keyword for keywords in df[\"keywords\"] for keyword in keywords]\n\n# Count keyword occurrences\nkeyword_counts = pd.Series(all_keywords).value_counts()\n\n# Display the top 10 most popular keywords\nprint(keyword_counts.head(10))","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***Answer 6:***\n- ***`USB` connectivity, `charging (especially fast charging)`, and `cables` are prominent product features.***\n- ***`Prepositions and conjunctions` like `\"with\", \"for\", \"and\", \"to\"` suggest a focus on explaining product compatibility and usage scenarios.***\n- ***`Cables and smart devices` are likely well-represented in the dataset.***\n- ***`Product names` tend to be concise and use `common words`, potentially benefiting from refined keyword extraction techniques.***","metadata":{}},{"cell_type":"markdown","source":"## ***Q7: What are the most popular product reviews?***","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob  # Import TextBlob library\n# Select review column\ndf[[\"product_id\", \"user_id\", \"review_content\"]]\n\n# Calculate sentiment score for each review\ndf[\"sentiment\"] = df[\"review_content\"].apply(lambda text: TextBlob(text).sentiment.polarity)\n\n# Sort by sentiment score (ascending for positive)\ndf_sorted = df.sort_values(by=\"sentiment\", ascending=True)\n\n# Display top reviews based on a desired number (e.g., top 10)\ntop_reviews = df_sorted.head(10)\nprint(top_reviews)","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***Answer 7:***\n- ***The `overall sentiment scores` are relatively low, suggesting a tendency towards neutral or slightly negative reviews in the sample.***\n\n- ***The `review with the highest sentiment` score is `\"I have installed this in my kitchen working fine\"` (product_id 1463) with a score of -0.170167, indicating a mildly positive sentiment.***\n\n- ***The `review with the lowest sentiment` score is `\"tv on off not working, so difficult to battery charge\"` (product_id 155) with a score of -0.600000, suggesting a strongly negative sentiment.***\n\n- ***Several reviews mention issues with battery charging `(product_id 155)`, product quality `(product_id 1237)`, and ease of use `(product_id 1198)`, highlighting potential areas for improvement.***\n\n- ***Some `reviews` express `both positive and negative` aspects within the same text, like `\"Like and happy,,Please don't buy this heater\"` (product_id 1237), suggesting a nuanced evaluation of the product.***\n\n- ***The `user_id column` seems to contain `commas`, indicating multiple user IDs for some reviews. This might need investigation to ensure accuracy.***\n\n- ***`Reviews` for `product_id 22, 152, and 723` have identical content, suggesting potential data duplication or errors.***","metadata":{}},{"cell_type":"markdown","source":"## ***Q8: What is the correlation between discounted_price and rating?***","metadata":{}},{"cell_type":"code","source":"# Calculate the correlation coefficient\ncorrelation_coefficient = df[\"discounted_price\"].corr(df[\"rating\"])\n\n# Print the correlation coefficient with two decimal places\nprint(f\"Correlation between discounted_price and rating: {correlation_coefficient:.2f}\")","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***Answer 8:*** \n***`Discounted price` and `rating` have a `weak positive correlation`. This means that products with higher discounted prices tend to have slightly higher ratings, but the relationship is not very strong.***","metadata":{}},{"cell_type":"markdown","source":"## ***Q9: What are the Top 5 categories based with highest ratings?***","metadata":{}},{"cell_type":"code","source":"# Group data by category and calculate average rating\naverage_ratings = df.groupby(\"category\")[\"rating\"].mean().reset_index()\n\n# Sort by average rating in descending order\naverage_ratings = average_ratings.sort_values(by=\"rating\", ascending=False)\n\n# Print the top 5 categories\nprint(\"Top 5 categories with highest average ratings:\")\nfor i in range(5):\n    category = average_ratings.iloc[i][\"category\"]\n    average_rating = average_ratings.iloc[i][\"rating\"]\n    print(f\"{i+1}. {category}: {average_rating:.2f}\")","metadata":{"execution":{"execution_failed":"2026-01-10T07:37:54.957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ***Answer 9:***\n- ***The `top 5 categories` have `average ratings` between `4.50 and 4.60`, indicating overall positive customer satisfaction within these areas.***\n- ***Most of the top-rated categories fall within technology-related domains, including `tablets, networking devices, photography accessories, media streaming devices, and calculators`.***\n- ***Within broader categories like `\"Computers & Accessories\" and \"Electronics,\"` specific subcategories emerge as particularly well-rated, such as `tablets, powerline adapters, film accessories, and streaming clients`.***\n- ***`Four categories` share a `rating of 4.50`, suggesting similar levels of customer satisfaction across these areas.***\n- ***The `presence of \"Basic Calculators\"` in the top 5 suggests that even relatively simple products can `achieve high ratings` if they meet customer needs effectively.***","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}}]}